Experiment_ID,Model_Name,Model_Type,Hyperparameters,Train_Accuracy,Val_Accuracy,Test_Accuracy,Test_Precision,Test_Recall,Test_F1,Training_Time_sec,Observations
EXP-001,Random Forest,Traditional ML,"n_estimators=50, random_state=42",1.0,0.9943181818181817,0.9954545454545455,0.9956709956709956,0.9954545454545455,0.9954517027687759,1.9339125156402588,"Baseline with 50 trees. Fewer trees, faster training."
EXP-002,Random Forest,Traditional ML,"n_estimators=100, random_state=42",1.0,0.9931818181818182,0.9954545454545455,0.9956709956709956,0.9954545454545455,0.9954517027687759,3.7135822772979736,Baseline with 100 trees. Good balance of accuracy and speed.
EXP-003,Random Forest,Traditional ML,"n_estimators=200, random_state=42",1.0,0.9943181818181819,0.9954545454545455,0.9956709956709956,0.9954545454545455,0.9954517027687759,7.2404890060424805,"Baseline with 200 trees. More trees, diminishing returns observed."
EXP-004,Logistic Regression,Traditional ML,"max_iter=200, C=0.1, random_state=42",0.94375,0.9318181818181819,0.9363636363636364,0.9393167532345692,0.9363636363636364,0.9353675141281629,0.7409660816192627,"Regularization strength C=0.1. Strong regularization, potential underfitting."
EXP-005,Logistic Regression,Traditional ML,"max_iter=200, C=1.0, random_state=42",0.9738636363636364,0.9676136363636363,0.9727272727272728,0.974021729904083,0.9727272727272728,0.9724640256149185,0.8114597797393799,"Regularization strength C=1.0. Default regularization, balanced performance."
EXP-006,Logistic Regression,Traditional ML,"max_iter=200, C=10.0, random_state=42",0.9840909090909091,0.9784090909090908,0.9795454545454545,0.9808759867583396,0.9795454545454545,0.9794660095103555,1.3910751342773438,"Regularization strength C=10.0. Weak regularization, may overfit on complex patterns."
EXP-007,KNN,Traditional ML,n_neighbors=3,0.9869318181818182,0.9698863636363637,0.9772727272727273,0.9790081789093646,0.9772727272727273,0.9770894225375225,0.12778568267822266,"Using 3 nearest neighbors. Fewer neighbors, more sensitive to noise."
EXP-008,KNN,Traditional ML,n_neighbors=5,0.9846590909090909,0.965340909090909,0.9795454545454545,0.9803555030827759,0.9795454545454545,0.9792832328133997,0.11198949813842773,"Using 5 nearest neighbors. Default setting, good bias-variance tradeoff."
EXP-009,KNN,Traditional ML,n_neighbors=10,0.9693181818181819,0.9539772727272726,0.9704545454545455,0.9726032202316788,0.9704545454545455,0.97019850643608,0.12918829917907715,"Using 10 nearest neighbors. More neighbors, smoother decision boundaries."
EXP-010,Sequential NN,Deep Learning,"layers=[32, 16], optimizer=adam, epochs=30, batch=32",0.9651988744735718,0.9630681872367859,0.9795454740524292,0.9806506624688444,0.9795454545454545,0.9795070141668346,12.779694318771362,Small network - 2 layers. Architecture: 32-16-22. Final training loss: 0.1237. 
EXP-011,Sequential NN,Deep Learning,"layers=[64, 32], optimizer=adam, epochs=30, batch=32",0.9786931872367859,0.9715909361839294,0.9772727489471436,0.9782237964056145,0.9772727272727273,0.9772243161203367,13.096430778503418,Medium network - 2 layers (baseline). Architecture: 64-32-22. Final training loss: 0.0683. 
EXP-012,Sequential NN,Deep Learning,"layers=[128, 64, 32], optimizer=adam, epochs=30, batch=32",0.9872159361839294,0.9744318127632141,0.9795454740524292,0.98084743539289,0.9795454545454545,0.9794984647359487,12.209457397460938,Large network - 3 layers. Architecture: 128-64-32-22. Final training loss: 0.0437. 
EXP-013,Functional NN,Deep Learning,"arch=128-64-22, dropout=0.0, optimizer=adam, epochs=30",0.9936079382896423,0.9886363744735718,0.9818181991577148,0.9834710743801653,0.9818181818181818,0.9817726133515606,11.676399946212769,"Dropout rate: 0.0. No regularization, may overfit."
EXP-014,Functional NN,Deep Learning,"arch=128-64-22, dropout=0.3, optimizer=adam, epochs=30",0.96875,0.9801136255264282,0.9818181991577148,0.9822510822510823,0.9818181818181818,0.9818124964466428,12.718604564666748,"Dropout rate: 0.3. Moderate regularization, good generalization."
EXP-015,Functional NN,Deep Learning,"arch=128-64-22, dropout=0.5, optimizer=adam, epochs=30",0.9502840638160706,0.9659090638160706,0.9772727489471436,0.9784753385439884,0.9772727272727273,0.9772270513336879,13.125854015350342,"Dropout rate: 0.5. Heavy regularization, may underfit."
